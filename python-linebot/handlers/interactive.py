import openai
import requests
import threading
import os
from linebot.models import TextSendMessage


# === ç­‰å¾…æç¤ºèª ===
def get_waiting_message(context="general_chat"):
    return {
        "general_chat": "æˆ‘æƒ³æƒ³æ€éº¼èªªæ¯”è¼ƒå¥½ ğŸ¤”"
    }.get(context, "ç¨ç­‰ä¸€ä¸‹ï¼Œæˆ‘æƒ³æƒ³çœ‹ ğŸ¤”")

# === GPT èƒŒæ™¯å›è¦†æ¨é€ï¼ˆæœ‰è¨˜æ†¶ï¼‰ ===
# === æ”¹è‰¯ç‰ˆ GPT èƒŒæ™¯å›è¦†æ¨é€ï¼ˆå«äº’å‹•è¿½è¹¤ï¼‰ ===
import traceback
import time

import traceback
import time

def gpt_push_response(context, user_id, user_text, system_prompt, line_bot_api, history_messages=None, retry_count=1):
    try:
        gpt_messages = [{"role": "system", "content": system_prompt}]
        if history_messages:
            cleaned = []
            for msg in history_messages:
                if isinstance(msg, dict) and msg.get("role") in ["user", "assistant"] and msg.get("content"):
                    cleaned.append({"role": msg["role"], "content": msg["content"]})
            gpt_messages += cleaned

        gpt_messages.append({"role": "user", "content": user_text})

        print(f"ğŸ›  [DEBUG] å‘¼å« GPTä¸­...")
        print(f"ğŸ›  [DEBUG] GPT å‚³é€è¨Šæ¯æ•¸é‡: {len(gpt_messages)}")

        response = openai.ChatCompletion.create(
            model="gpt-4o",
            messages=gpt_messages,
            timeout=30
        )

        print(f"ğŸ›  [DEBUG] GPT å›å‚³ raw: {response}")

        if not response or "choices" not in response or not response["choices"]:
            raise ValueError("GPT å›å‚³ç©ºçš„ choicesï¼Œç„¡æ³•ç”¢ç”Ÿå›è¦†")

        reply_text = response["choices"][0]["message"]["content"].strip()
        print(f"ğŸ›  [DEBUG] GPT å›è¦†å…§å®¹: {reply_text}")

        # æ¨é€LINEå‰ä¹Ÿç¢ºèª
        print(f"ğŸ›  [DEBUG] æº–å‚™æ¨é€åˆ° LINE UserID: {user_id}")

        line_bot_api.push_message(user_id, TextSendMessage(text=reply_text))
        print(f"âœ… [DEBUG] æˆåŠŸæ¨é€åˆ° LINE")

    except Exception as e:
        traceback.print_exc()
        print(f"âŒ [DEBUG] ç™¼ç”Ÿä¾‹å¤–éŒ¯èª¤ ({type(e).__name__}): {e}")

        if retry_count > 0:
            print(f"ğŸ”„ [DEBUG] å˜—è©¦é‡æ–°å‘¼å« GPTï¼Œå‰©é¤˜é‡è©¦æ¬¡æ•¸: {retry_count}")
            time.sleep(2)
            gpt_push_response(context, user_id, user_text, system_prompt, line_bot_api, history_messages, retry_count=retry_count-1)
        else:
            print("ğŸš¨ [DEBUG] é‡è©¦å¾Œä»å¤±æ•—ï¼Œå˜—è©¦æ¨é€éŒ¯èª¤æç¤ºè¨Šæ¯")
            try:
                line_bot_api.push_message(user_id, TextSendMessage(text="å“å‘€ï¼Œæˆ‘é€™é‚Šå‡ºäº†ä¸€é»å•é¡Œï¼Œè«‹å†å•æˆ‘ä¸€æ¬¡ ğŸ™"))
            except Exception as push_err:
                print(f"âŒ [DEBUG] æ¨é€éŒ¯èª¤è¨Šæ¯åˆ°LINEä¹Ÿå¤±æ•—: {push_err}")


# === ğŸ—¨ï¸ äº’å‹•å¼æ¨¡å¼è™•ç†ä¸»å‡½å¼ ===
# === ğŸ—¨ï¸ æ”¹è‰¯ç‰ˆäº’å‹•å¼æ¨¡å¼è™•ç†ä¸»å‡½å¼ ===
def handle_interactive_mode(event, user_id, user_text, line_bot_api, history):
    # ğŸ›  ä¿®æ­£ç‰ˆï¼šæ­£ç¢ºå»ºæ§‹æœ‰ role çš„æ­·å²è³‡æ–™
    messages = [{"role": "system", "content": "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ C èªè¨€å­¸ç¿’åŠ©æ•™ï¼Œæ“…é•·æ ¹æ“šä¸Šä¸‹æ–‡é€²è¡Œå›ç­”ï¼Œé¿å…é‡è¤‡ä¸»é¡Œã€‚"}]
    
    for msg in sorted(history.get("messages", []), key=lambda x: x.get("timestamp", "")):
        if msg.get("message_text"):
            messages.append({"role": "user", "content": msg["message_text"]})
        elif msg.get("bot_response"):
            messages.append({"role": "assistant", "content": msg["bot_response"]})

    # ğŸ”¥ é€™æ™‚ messages å°±æ˜¯å®Œæ•´æ­·å²ï¼šæœ‰ userã€æœ‰ bot
    short_history = messages[-4:]  # æœ€è¿‘å››ç­†æœ‰ç”¨äº’å‹•
    
    # é€å‡ºç­‰å¾…æç¤º
    wait_msg = get_waiting_message("general_chat")
    line_bot_api.reply_message(event.reply_token, TextSendMessage(text=wait_msg))

    # åˆ¤æ–·äº’å‹•æƒ…å¢ƒ
    context = "interactive_learning"

    # è¨­è¨ˆæ›´äº’å‹•å¼ prompt
    system_prompt = """
ä½ æ˜¯ä¸€ä½è¦ªåˆ‡ã€æœ‰è€å¿ƒçš„ C èªè¨€å­¸ç¿’æ•™ç·´ï¼Œç›®æ¨™æ˜¯ä¿ƒé€²å­¸ç”Ÿä¸»å‹•å­¸ç¿’å’Œå»ºè¨­æ€§å°è©±ã€‚

ğŸŸ¢ å¦‚æœå­¸ç”Ÿä¸»å‹•æå•ï¼šç°¡å–®è§£é‡‹ + èˆ‰ä¾‹ + æå•ï¼ˆé¼“å‹µå­¸ç”Ÿå»¶ä¼¸è‡ªå·±çš„ä¾‹å­æˆ–æƒ³æ³•ï¼‰
    - èªæ°£è¼•é¬†ï¼Œåƒæœ‹å‹èŠå¤©ã€‚
    - æœ€å¾Œç”¨ä¸€å¥å¼•å°å•é¡Œï¼Œæ¯”å¦‚ï¼šã€Œä½ å¯ä»¥è©¦è‘—å¯«ä¸€å€‹é¡ä¼¼çš„å—ï¼Ÿã€ã€ã€Œé‚£å¦‚æœæ”¹æˆXXXæœƒæ€æ¨£ï¼Ÿã€

ğŸ”µ å¦‚æœå­¸ç”Ÿæ²’æœ‰å…·é«”æå•ï¼šä¸»å‹•çµ¦ä¸€å€‹ç°¡å–®å°æŒ‘æˆ°æˆ–ä¿®æ”¹ä»»å‹™ã€‚
    - é¡Œç›®è¦æœ‰é–‹æ”¾æ€§ï¼Œå¼•å°å­¸ç”Ÿæ€è€ƒä¸åŒåšæ³•ã€‚
    - æ¯æ¬¡åªçµ¦ä¸€é»æç¤ºï¼Œæ ¹æ“šå­¸ç”Ÿå›è¦†èª¿æ•´é›£åº¦ã€‚

âš¡ ç‰¹åˆ¥æ³¨æ„ï¼š
    - å¼•å°å­¸ç”Ÿã€å…·é«”å›ç­”ã€‘ï¼Œä¾‹å¦‚ï¼šè‡ªå·±å¯«ç¨‹å¼ç‰‡æ®µã€èˆ‰ç”Ÿæ´»ä¾‹å­ã€è§£é‡‹è‡ªå·±çš„ç†è§£ã€‚
    - äº’å‹•éç¨‹è¦æœ‰3æ¬¡ä»¥ä¸Šçš„ä¾†å›æ‰ç®—ä¸€æ¬¡å®Œæ•´äº’å‹•ã€‚
    - é‡å°å­¸ç”Ÿå›æ‡‰å…§å®¹ï¼Œçµ¦å‡ºæ­£å‘å›é¥‹æˆ–è¿½å•ç´°ç¯€ã€‚

è«‹ç”¨é€™å€‹äº’å‹•ç­–ç•¥å›æ‡‰å­¸ç”Ÿï¼
    """

    # é–‹èƒŒæ™¯åŸ·è¡Œï¼Œæ¨é€ GPT å›è¦†
    threading.Thread(
        target=gpt_push_response,
        args=(context, user_id, user_text, system_prompt, line_bot_api, short_history)
    ).start()
