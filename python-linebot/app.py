from flask import Flask, request, abort, jsonify
from linebot import LineBotApi, WebhookHandler
from linebot.exceptions import InvalidSignatureError
from linebot.models import *
import os, openai
import requests

app = Flask(__name__)
line_bot_api = LineBotApi(os.getenv('CHANNEL_ACCESS_TOKEN'))
handler = WebhookHandler(os.getenv('CHANNEL_SECRET'))
openai.api_key = os.getenv('OPENAI_API_KEY')
NODE_SERVER_URL = "https://node-mongo-b008.onrender.com"
user_mode = {}
user_state = {}  # user_id: { "mode": "active", "last_question": "...", "awaiting_answer": True }

def load_history(user_id):
    url = f"{NODE_SERVER_URL}/get_history"
    try:
        response = requests.get(url, params={"user_id": user_id, "limit": 10}, timeout=10)
        response.raise_for_status()
        data = response.json()
        return data if "messages" in data else {"messages": []}
    except requests.exceptions.RequestException as e:
        print(f"âŒ API è®€å–å¤±æ•—: {e}")
        return {"messages": []}



@app.route("/callback", methods=['POST'])
def callback():
    signature = request.headers.get('X-Line-Signature')
    if not signature:
        abort(403)

    body = request.get_data(as_text=True)
    print("ğŸ“¥ æ”¶åˆ° LINE Webhook:", body)

    try:
        handler.handle(body, signature)
    except InvalidSignatureError:
        print("âŒ LINE ç°½åé©—è­‰å¤±æ•—")
        abort(400)

    # **è§£æ JSON ä¸¦å„²å­˜åˆ° MongoDB**
    data = request.get_json(silent=True)
    if not data or "events" not in data:
        return jsonify({"error": "Invalid data"}), 400

    for event in data["events"]:
        if event["type"] == "message":
            message_data = {
                "user_id": event["source"].get("userId", "Unknown"),
                "message_text": event["message"].get("text", ""),
                "message_type": event["message"].get("type", "unknown")
            }
            print("ğŸ“© LINE è¨Šæ¯:", message_data)

            # **å®‰å…¨åœ°å‚³é€åˆ° Node.js**
            try:
                response = requests.post(f"{NODE_SERVER_URL}/save_message", json=message_data)
                print("ğŸ“¤ ç™¼é€è‡³ Node.js:", response.status_code, response.text)
            except requests.exceptions.RequestException as e:
                print(f"âŒ å„²å­˜å¤±æ•—: {e}")

    return 'OK'

def send_mode_selection(user_id):
    flex_message = FlexSendMessage(
        alt_text="è«‹é¸æ“‡å­¸ç¿’æ¨¡å¼",
        contents={
            "type": "bubble",
            "body": {
                "type": "box",
                "layout": "vertical",
                "contents": [
                    {"type": "text", "text": "è«‹é¸æ“‡å­¸ç¿’æ¨¡å¼", "weight": "bold", "size": "lg"},
                    {"type": "button", "style": "primary", "color": "#1DB446",
                     "action": {"type": "message", "label": "äº’å‹•å¼ (Interactive)", "text": "mode_interactive"}},
                    {"type": "button", "style": "primary", "color": "#FFB74D",
                     "action": {"type": "message", "label": "å»ºæ§‹å¼ (Constructive)", "text": "mode_constructive"}},
                    {"type": "button", "style": "primary", "color": "#42A5F5",
                     "action": {"type": "message", "label": "ä¸»å‹•å¼ (Active)", "text": "mode_active"}},
                    {"type": "button", "style": "primary", "color": "#9E9E9E",
                     "action": {"type": "message", "label": "è¢«å‹•å¼ (Passive)", "text": "mode_passive"}}
                ]
            }
        }
    )
    line_bot_api.push_message(user_id, flex_message)

# ç”¢ç”Ÿä¸»å‹•å­¸ç¿’çš„å•é¡Œ
def generate_active_question():
    prompt = """
ä½ æ˜¯ä¸€ä½ C èªè¨€çš„å­¸ç¿’è¼”å°è€å¸«ï¼Œè«‹ä½ è¨­è¨ˆä¸€å€‹èˆ‡ C èªè¨€ç›¸é—œçš„æŒ‘æˆ°æ€§å•é¡Œï¼Œè®“å­¸ç¿’è€…å¯ä»¥æ€è€ƒä¸¦å˜—è©¦å›ç­”ã€‚å•é¡Œæ‡‰æ¶µè“‹ C èªè¨€çš„æ ¸å¿ƒæ¦‚å¿µï¼Œä¾‹å¦‚ï¼šè¨˜æ†¶é«”ç®¡ç†ã€æŒ‡æ¨™ã€çµæ§‹ã€é™£åˆ—ã€æµç¨‹æ§åˆ¶ã€å‡½å¼ã€æˆ–å­—ä¸²æ“ä½œç­‰ï¼Œé›£åº¦é©ä¸­ï¼Œæœ‰åŠ©æ–¼ç†è§£èªæ³•èˆ‡é‚è¼¯ã€‚

è«‹åªç”¢ç”Ÿå•é¡Œï¼Œä¸è¦é™„åŠ ç­”æ¡ˆã€‚
"""
    response = openai.ChatCompletion.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€å€‹ C èªè¨€æ•™å­¸åŠ©æ‰‹ï¼Œæœƒä¸»å‹•æå‡ºå…·æŒ‘æˆ°æ€§çš„å•é¡Œã€‚"},
            {"role": "user", "content": prompt}
        ]
    )
    return response["choices"][0]["message"]["content"].strip()

# ç”¢ç”Ÿäº’å‹•å¼å°è©±
def generate_interactive_response(user_input):
    response = openai.ChatCompletion.create(
        model="gpt-4o",
        messages=[{"role": "system", "content": "ä½ æ˜¯ä¸€å€‹å°è©±å‹å­¸ç¿’åŠ©ç†ï¼Œæœƒæ ¹æ“šä½¿ç”¨è€…çš„å•é¡Œé€²è¡Œäº’å‹•ã€‚"},
                  {"role": "user", "content": user_input}]
    )
    return response["choices"][0]["message"]["content"]

# ç”¢ç”Ÿå¼•å°å¼å•é¡Œ (å»ºæ§‹æ¨¡å¼)
def generate_constructive_prompt(user_input):
    prompt = f"""
ä½¿ç”¨è€…è¼¸å…¥äº†ä»¥ä¸‹å…§å®¹ï¼š

ã€Œ{user_input}ã€

ä½ æ˜¯ C èªè¨€æ•™å­¸åŠ©ç†ï¼Œè«‹æ ¹æ“šé€™æ®µå…§å®¹ï¼Œæå‡ºä¸€å€‹ã€Œå…·å•Ÿç™¼æ€§ã€çš„è¿½å•ï¼Œå¼•å°ä½¿ç”¨è€…ï¼š

- è§£é‡‹è‡ªå·±çš„è§€é»
- è£œå……ç´°ç¯€æˆ–ä¾‹å­
- é€²ä¸€æ­¥æ€è€ƒå…¶ä»–å¯èƒ½æ€§
- æˆ–é‡æ§‹ä»–å‰›å‰›çš„ç†è§£

è«‹åªçµ¦ä¸€å¥å…·é«”ã€è‡ªç„¶çš„è¿½å•ï¼Œä¾‹å¦‚ï¼š
- ä½ é€™æ¨£è¨­è¨ˆçš„åŸå› æ˜¯ä»€éº¼ï¼Ÿ
- æœ‰å…¶ä»–æ–¹å¼å¯ä»¥é”åˆ°åŒæ¨£æ•ˆæœå—ï¼Ÿ
- é€™æ®µç¨‹å¼åœ¨ä»€éº¼æƒ…æ³ä¸‹æœƒå‡ºéŒ¯ï¼Ÿ
"""
    response = openai.ChatCompletion.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½æ“…é•·å¼•å°å­¸ç¿’çš„ C èªè¨€åŠ©æ•™ã€‚"},
            {"role": "user", "content": prompt}
        ]
    )
    return response["choices"][0]["message"]["content"].strip()


def is_c_language(text):
    c_keywords = ["c", "#include", "int ", "void ", "printf(", "return", "malloc", "struct "]
    text = text.lower()  # è½‰æ›ç‚ºå°å¯«ï¼Œé¿å…å¤§å°å¯«ä¸åŒ¹é…
    return any(keyword in text for keyword in c_keywords)

def GPT_response(messages):
    if not isinstance(messages, list) or len(messages) == 0:
        raise ValueError("messages å¿…é ˆæ˜¯ä¸€å€‹åŒ…å«å­—å…¸çš„åˆ—è¡¨")
    if messages[0].get("role") != "system":
        messages.insert(0, {"role": "system", "content": "ä½ åªèƒ½ä½¿ç”¨ç¹é«”ä¸­æ–‡æˆ–è‹±æ–‡å›ç­”ã€‚"})
    model = "ft:gpt-4o-2024-08-06:personal::B5sbnkYa" if is_c_language(messages[-1].get("content", "")) else "gpt-4o"
    print(f"ä½¿ç”¨æ¨¡å‹: {model}")
    response = openai.ChatCompletion.create(
        model=model,
        messages=messages,
        max_tokens=500,
        timeout=30  # é¿å… API è¶…æ™‚
    )
    return response["choices"][0]["message"]["content"].strip()

def is_answer_related(user_input, last_question):
    """åˆ¤æ–·ä½¿ç”¨è€…è¼¸å…¥æ˜¯å¦èˆ‡ä¸Šä¸€é¡Œæœ‰é—œè¯"""
    user_input = user_input.lower()
    keywords = ["ç­”æ¡ˆ", "æ˜¯ä»€éº¼", "ä¸æ‡‚", "ç‚ºä»€éº¼", "æˆ‘è¦ºå¾—", "æˆ‘çŒœ", "å¯èƒ½", "å› ç‚º", "æ‡‰è©²"]
    return any(kw in user_input for kw in keywords) or is_c_language(user_input)

@handler.add(MessageEvent, message=TextMessage)
def handle_message(event):
    user_id = event.source.user_id
    user_text = event.message.text.strip()
    print(f"ğŸ’¬ æ”¶åˆ°ä¾†è‡ª {user_id} çš„è¨Šæ¯: {user_text}")
    mode_map = {
        "mode_passive": "passive",
        "mode_active": "active",
        "mode_constructive": "constructive",
        "mode_interactive": "interactive"
    }

    if user_text in mode_map:
        user_mode[user_id] = mode_map[user_text]
    
        descriptions = {
            "passive": "ä½ æœƒä»¥é–±è®€ç‚ºä¸»ï¼Œæˆ‘æœƒç›¡é‡ç°¡æ½”åœ°å›ç­”ä½ ï¼Œä¸ä¸»å‹•æå•ã€‚",
            "active": "æˆ‘æœƒçµ¦ä½ ä¸€äº›æŒ‘æˆ°æ€§çš„å•é¡Œï¼Œè®“ä½ ä¸»å‹•æ€è€ƒå’Œä½œç­”ã€‚",
            "constructive": "æˆ‘æœƒæ ¹æ“šä½ çš„å›ç­”ï¼Œé€²ä¸€æ­¥è¿½å•ï¼Œå¹«åŠ©ä½ æ·±åŒ–æƒ³æ³•ã€‚",
            "interactive": "æˆ‘å€‘æœƒåƒæœ‹å‹ä¸€æ¨£å°è©±ï¼Œä¸€èµ·è¨è«–ä¸»é¡Œå’Œè§€é»ã€‚"
        }
    
        mode_key = mode_map[user_text]
        mode_name = user_text.replace("mode_", "").capitalize()
        description = descriptions[mode_key]
    
        if mode_key == "active":
            question = generate_active_question()
            user_state[user_id] = {
                "mode": "active",
                "last_question": question,
                "awaiting_answer": True
            }
            reply_text = f"âœ… å·²åˆ‡æ›è‡³ã€{mode_name}ã€æ¨¡å¼\n\n{description}\n\nğŸ§  ç¬¬ä¸€é¡Œï¼š{question}\n\nä½ è¦ºå¾—ç­”æ¡ˆæ˜¯ä»€éº¼ï¼Ÿ"
        else:
            reply_text = f"âœ… å·²åˆ‡æ›è‡³ã€{mode_name}ã€æ¨¡å¼\n\n{description}"
    
        line_bot_api.reply_message(event.reply_token, TextSendMessage(reply_text))
        return

    # **ğŸ“Œ å–å¾—ä½¿ç”¨è€…ç•¶å‰æ¨¡å¼ï¼Œé è¨­ç‚ºè¢«å‹•æ¨¡å¼**
    mode = user_mode.get(user_id, "passive")
    print(f"ğŸ›  ç”¨æˆ¶ {user_id} çš„ç›®å‰æ¨¡å¼ï¼š{mode}")

    history = load_history(user_id)
    messages = [{"role": "system", "content": "ä½ æ˜¯ä¸€å€‹æ™ºæ…§åŠ©ç†ï¼Œè«‹è¨˜ä½ä½¿ç”¨è€…çš„å°è©±æ­·å²ã€‚"}]
    for msg in history.get("messages", [])[-10:]:
        if msg.get("message_text") and msg.get("bot_response"):
            messages.append({"role": "user", "content": msg["message_text"]})
            messages.append({"role": "assistant", "content": msg["bot_response"]})
    messages.append({"role": "user", "content": user_text})

    # **ğŸ“Œ æ ¹æ“šæ¨¡å¼ä¾†é¸æ“‡ AI äº’å‹•æ–¹å¼**
    if mode in ["passive", "interactive"]:
        response_text = GPT_response(messages)
    elif mode == "active":
        state = user_state.get(user_id, {})
        last_q = state.get("last_question")
        awaiting = state.get("awaiting_answer", False)
    
        def is_asking_for_answer(user_input):
            user_input = user_input.lower()
            return any(kw in user_input for kw in ["ç­”æ¡ˆ", "æ­£ç¢º", "æ˜¯ä»€éº¼", "è§£ç­”", "å‘Šè¨´æˆ‘"])
    
        def wants_next_question(user_input):
            user_input = user_input.lower()
            return any(kw in user_input for kw in ["ä¸‹ä¸€é¡Œ", "ä¸‹ä¸€å€‹", "å†ä¸€é¡Œ", "è«‹å†çµ¦ä¸€é¡Œ", "å†ä¾†", "ä¸‹ä¸€"])
    
        if awaiting and last_q:
            if is_asking_for_answer(user_text):
                # ä½¿ç”¨è€…åœ¨å•ç­”æ¡ˆ
                answer_prompt = f"""è«‹é‡å°ä»¥ä¸‹ C èªè¨€å•é¡Œçµ¦å‡ºç°¡å–®æ˜ç¢ºçš„è§£é‡‹èˆ‡ç­”æ¡ˆï¼š
    
    å•é¡Œï¼šã€Œ{last_q}ã€
    """
                response = openai.ChatCompletion.create(
                    model="gpt-4o",
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ C èªè¨€æ•™å­¸åŠ©ç†ï¼Œè«‹ç”¨ç°¡å–®æ–¹å¼æä¾›æ˜ç¢ºè§£ç­”ã€‚"},
                        {"role": "user", "content": answer_prompt}
                    ]
                )
                response_text = response["choices"][0]["message"]["content"].strip()
                user_state[user_id]["awaiting_answer"] = False
                user_state[user_id]["last_question"] = None
    
            elif is_answer_related(user_text, last_q):
                # ä½¿ç”¨è€…åœ¨å›ç­”å•é¡Œ
                answer_prompt = f"""ä»¥ä¸‹æ˜¯ä½ å…ˆå‰å•çš„ C èªè¨€å•é¡Œï¼š
    ã€Œ{last_q}ã€
    
    ä½¿ç”¨è€…å›è¦†ï¼šã€Œ{user_text}ã€
    
    è«‹é‡å°ä»–çš„å›ç­”çµ¦å‡ºå›é¥‹ï¼ˆä¸çµ¦ç­”æ¡ˆï¼‰ï¼Œå¯é¼“å‹µã€ä¿®æ­£éŒ¯èª¤ã€å¼•å°æ€è€ƒã€‚
    """
                response = openai.ChatCompletion.create(
                    model="gpt-4o",
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ C èªè¨€åŠ©æ•™ï¼Œè«‹é‡å°ä½¿ç”¨è€…çš„å›ç­”é€²è¡Œå»ºè¨­æ€§å›é¥‹ã€‚"},
                        {"role": "user", "content": answer_prompt}
                    ]
                )
                response_text = response["choices"][0]["message"]["content"].strip()
                user_state[user_id]["awaiting_answer"] = False
                user_state[user_id]["last_question"] = None
    
            elif wants_next_question(user_text):
                # ä½¿ç”¨è€…æ˜ç¢ºè¦æ±‚ä¸‹ä¸€é¡Œ
                question = generate_active_question()
                response_text = f"ğŸ§  æ–°æŒ‘æˆ°ä¾†å›‰ï¼\n\nâ“{question}\n\nä½ è¦ºå¾—ç­”æ¡ˆæ˜¯ä»€éº¼ï¼Ÿ"
                user_state[user_id] = {
                    "mode": "active",
                    "last_question": question,
                    "awaiting_answer": True
                }
    
            else:
                # ä¸æ˜¯æ˜ç¢ºå›ç­”ï¼Œä¹Ÿä¸æ˜¯æ˜ç¢ºè¦ä¸‹ä¸€é¡Œï¼Œæç¤ºç”¨æˆ¶é¸æ“‡
                response_text = "ğŸ“ ä½ é‚„æ²’å®Œæˆä¸Šä¸€é¡Œå–”ï½å¦‚æœä½ æƒ³çŸ¥é“ç­”æ¡ˆï¼Œå¯ä»¥å•æˆ‘ã€Œé€™é¡Œç­”æ¡ˆæ˜¯ä»€éº¼ï¼Ÿã€ï¼›å¦‚æœæƒ³æŒ‘æˆ°ä¸‹ä¸€é¡Œï¼Œè«‹èªªã€Œä¸‹ä¸€é¡Œã€ï½"
    
        else:
            # æ²’æœ‰å•é¡Œåœ¨ç­‰å¾…ä¸­ï¼Œå‡ºæ–°é¡Œ
            question = generate_active_question()
            response_text = f"ğŸ§  ä¾†æŒ‘æˆ°çœ‹çœ‹é€™é¡Œå§ï¼š\n\nâ“{question}\n\nä½ è¦ºå¾—ç­”æ¡ˆæ˜¯ä»€éº¼ï¼Ÿ"
            user_state[user_id] = {
                "mode": "active",
                "last_question": question,
                "awaiting_answer": True
            }

    elif mode == "constructive":
        response_text = generate_constructive_prompt(user_text)
    else:
        response_text = "æœªçŸ¥æ¨¡å¼ï¼Œè«‹é‡æ–°é¸æ“‡ã€‚"
        
    line_bot_api.reply_message(event.reply_token, TextSendMessage(response_text))
    if response_text.strip():
        message_data = {"user_id": user_id, "message_text": user_text, "bot_response": response_text, "message_type": "text"}
        try:
            requests.post(f"{NODE_SERVER_URL}/save_message", json=message_data, timeout=10)
            print(f"âœ… æˆåŠŸå„²å­˜å°è©±: {message_data}")
        except requests.exceptions.RequestException as e:
            print(f"âŒ å„²å­˜å°è©±å¤±æ•—: {e}")

if __name__ == "__main__":
    port = int(os.environ.get('PORT', 5000))
    app.run(host='0.0.0.0', port=port)
